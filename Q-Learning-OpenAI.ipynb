{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNotSlippery-v0', # name given to this new environment\n",
    "        entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', # env entry point\n",
    "        kwargs={'map_name': '4x4', 'is_slippery': False}, # argument passed to the env\n",
    "        max_episode_steps = 100,\n",
    "        reward_threshold = 0.78, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    print(\"Already registered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery-v0') # customized version of FrozenLake\n",
    "env.reset()\n",
    "\n",
    "for step in range(15):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time.sleep(0.3)\n",
    "    clear_output(wait=True) # clearing the output in order to re render the env\n",
    "    if done:\n",
    "        env.reset()\n",
    "    \n",
    "env.close()\n",
    "# all the actions doesn't matter coz it fell into the hole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation # observation or state is at position 4, we call it observation as used in OpenAI but in AI we say state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Initial Hyperparameter and Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size = env.action_space.n # returns no. of available actions i.e 4 - up down left and right\n",
    "state_size = env.observation_space.n # returns no. of possible states in a 4x4 i.e 16\n",
    "# rows--States  columns--Actions?\n",
    "q_table = np.zeros([state_size, action_size])\n",
    "q_table # 4 columns for actions and 16 rows for states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20000 # episodes - no. of times agent plays the game i.e. it has to hit done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.8 # learning rate - if set very low then it will take a long time for training; if set too high \n",
    "            # it may never actually converge. It needs to be set in balanced\n",
    "GAMMA = 0.95 # DISCOUNT RATE recall is going to be multiplied with rewards ; gamma^2 r + gamma^3 ...\n",
    "            # more gamma more rewards; less gamma less rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 # default exploration rate\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.001 # exponential decay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Functions\n",
    "#### The critical 3 update functions for our Q-Table learning process\n",
    "####     - Epsilon-Greedy Action Selection\n",
    "####     - Computation of Next Q Value\n",
    "####     - Epsilon Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
    "    # discrete_state is betwee 0 and 15\n",
    "    \n",
    "    random_number = np.random.random()\n",
    "    \n",
    "    # EXPLOITATION (choose the action that maximizes Q)\n",
    "    if random_number > epsilon:\n",
    "        # initially epsilon is 1.0; as we play more and more game, then epsilon will decay with a rate 0.001 and \n",
    "        # eventually random no. [0,1) will be greater than epsilon\n",
    "        \n",
    "        state_row = q_table[discrete_state,:] # rows are states ina Q table\n",
    "        \n",
    "        action = np.argmax(state_row)\n",
    "        # action is 0,1,2,3 ie. the index associated with the max Q value\n",
    "        \n",
    "    \n",
    "    # EXPLORATION (choose a random action)\n",
    "    else: # happens in the beginning\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,1,2,3\n",
    "s_row = np.array([0,0.1,0,0]) # ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(s_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_q_value(old_q_value,reward,next_optimal_q_value):\n",
    "    return old_q_value + ALPHA * (reward + GAMMA*next_optimal_q_value - old_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rduce_epsilon(epsilon,epoch):\n",
    "    return min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275.0\n",
      "1013.0\n",
      "1901.0\n",
      "2861.0\n",
      "3846.0\n",
      "4829.0\n",
      "5816.0\n",
      "6803.0\n",
      "7795.0\n",
      "8780.0\n",
      "9774.0\n",
      "10760.0\n",
      "11752.0\n",
      "12736.0\n",
      "13729.0\n",
      "14717.0\n",
      "15710.0\n",
      "16696.0\n",
      "17685.0\n",
      "18678.0\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "log_interval = 1000\n",
    "\n",
    "for episode in range(EPOCHS):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    # Agent plays the game\n",
    "    while not done: # not got into hole or reached the goal\n",
    "        \n",
    "        # take an ACTION\n",
    "        action = epsilon_greedy_action_selection(epsilon, q_table, state)\n",
    "        \n",
    "        # state, reward.. env.step()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # OLD (Current) Q VALUE Q(st, at)\n",
    "        old_q_value = q_table[state,action]\n",
    "        \n",
    "        # Get next optimal Q Value (max Q value for this state) Q(st+1, at+1)\n",
    "        next_optimal_q_value = np.max(q_table[new_state,:])\n",
    "        \n",
    "        # Compute the next Q Value\n",
    "        next_q = compute_next_q_value(old_q_value,reward,next_optimal_q_value)\n",
    "        \n",
    "        # Update the table\n",
    "        q_table[state,action] = next_q\n",
    "        \n",
    "        # track rewards\n",
    "        total_rewards = total_rewards + reward\n",
    "        \n",
    "        # new state is the state\n",
    "        state = new_state\n",
    "        \n",
    "    # Agent finished a round of the game\n",
    "    episode += 1\n",
    "    epsilon = rduce_epsilon(epsilon, episode)\n",
    "    \n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "    if episode % log_interval == 0:\n",
    "        print(np.sum(rewards))\n",
    "    \n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization and Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.76469819, 0.77162206],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for steps in range(100):\n",
    "    env.render()\n",
    "    action = np.argmax(q_table[state,:])\n",
    "    state,reward,done,info = env.step(action)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
